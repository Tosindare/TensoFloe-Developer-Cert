{"cells":[{"cell_type":"markdown","source":"**Exercise 1**\n\nCat and Dog Image classification using CNN ","metadata":{"tags":[],"cell_id":"00001-c34c65d6-152a-41c0-9ca7-a6dbd531e11e","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"# Startimport os\nimport zipfile\nimport random\nimport tensorflow as tf\nfrom tensorflow.keras.optimizers import RMSprop\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom shutil import copyfile\n\n# If the URL doesn't work, visit https://www.microsoft.com/en-us/download/confirmation.aspx?id=54765\n# And right click on the 'Download Manually' link to get a new URL to the dataset\n\n# Note: This is a very large dataset and will take time to download\n\n!wget --no-check-certificate \\\n    \"https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_3367a.zip\" \\\n    -O \"/tmp/cats-and-dogs.zip\"\n\nlocal_zip = '/tmp/cats-and-dogs.zip'\nzip_ref   = zipfile.ZipFile(local_zip, 'r')\nzip_ref.extractall('/tmp')\nzip_ref.close()\n","metadata":{"tags":[],"cell_id":"00001-2daac677-126b-47d5-b529-f0bbb0fbcc8d","deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(len(os.listdir('/tmp/PetImages/Cat/')))\nprint(len(os.listdir('/tmp/PetImages/Dog/')))\n\n# Expected Output:\n# 12501\n# 12501","metadata":{"tags":[],"cell_id":"00002-3f62ee34-6d88-4dce-8405-bf15288bc6ff","deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"try:\n    os.mkdir('/tmp/cats-v-dogs')\n    os.mkdir('/tmp/cats-v-dogs/training')\n    os.mkdir('/tmp/cats-v-dogs/testing')\n    os.mkdir('/tmp/cats-v-dogs/training/cats')\n    os.mkdir('/tmp/cats-v-dogs/training/dogs')\n    os.mkdir('/tmp/cats-v-dogs/testing/cats')\n    os.mkdir('/tmp/cats-v-dogs/testing/dogs')\nexcept OSError:\n    pass","metadata":{"tags":[],"cell_id":"00003-ed5ffae5-f813-4b15-a027-ac1f8efefdea","deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def split_data(SOURCE, TRAINING, TESTING, SPLIT_SIZE):\n    files = []\n    for filename in os.listdir(SOURCE):\n        file = SOURCE + filename\n        if os.path.getsize(file) > 0:\n            files.append(filename)\n        else:\n            print(filename + \" is zero length, so ignoring.\")\n\n    training_length = int(len(files) * SPLIT_SIZE)\n    testing_length = int(len(files) - training_length)\n    shuffled_set = random.sample(files, len(files))\n    training_set = shuffled_set[0:training_length]\n    testing_set = shuffled_set[-testing_length:]\n\n    for filename in training_set:\n        this_file = SOURCE + filename\n        destination = TRAINING + filename\n        copyfile(this_file, destination)\n\n    for filename in testing_set:\n        this_file = SOURCE + filename\n        destination = TESTING + filename\n        copyfile(this_file, destination)\n\n\n\nCAT_SOURCE_DIR = \"/tmp/PetImages/Cat/\"\nTRAINING_CATS_DIR = \"/tmp/cats-v-dogs/training/cats/\"\nTESTING_CATS_DIR = \"/tmp/cats-v-dogs/testing/cats/\"\nDOG_SOURCE_DIR = \"/tmp/PetImages/Dog/\"\nTRAINING_DOGS_DIR = \"/tmp/cats-v-dogs/training/dogs/\"\nTESTING_DOGS_DIR = \"/tmp/cats-v-dogs/testing/dogs/\"\n\nsplit_size = .9\nsplit_data(CAT_SOURCE_DIR, TRAINING_CATS_DIR, TESTING_CATS_DIR, split_size)\nsplit_data(DOG_SOURCE_DIR, TRAINING_DOGS_DIR, TESTING_DOGS_DIR, split_size)\n\n# Expected output\n# 666.jpg is zero length, so ignoring\n# 11702.jpg is zero length, so ignoring\n\nprint(len(os.listdir('/tmp/cats-v-dogs/training/cats/')))\nprint(len(os.listdir('/tmp/cats-v-dogs/training/dogs/')))\nprint(len(os.listdir('/tmp/cats-v-dogs/testing/cats/')))\nprint(len(os.listdir('/tmp/cats-v-dogs/testing/dogs/')))\n\n# Expected output:\n# 11250\n# 11250\n# 1250\n# 1250","metadata":{"tags":[],"cell_id":"00004-68a64dfc-f8eb-46a9-8f70-a728633fcc3c","deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"TRAINING_DIR = \"/tmp/cats-v-dogs/training/\"\ntrain_datagen = ImageDataGenerator(rescale=1.0/255.)\ntrain_generator = train_datagen.flow_from_directory(TRAINING_DIR,\n                                                    batch_size=100,\n                                                    class_mode='binary',\n                                                    target_size=(150, 150))\n\nVALIDATION_DIR = \"/tmp/cats-v-dogs/testing/\"\nvalidation_datagen = ImageDataGenerator(rescale=1.0/255.)\nvalidation_generator = validation_datagen.flow_from_directory(VALIDATION_DIR,\n                                                              batch_size=100,\n                                                              class_mode='binary',\n                                                              target_size=(150, 150))\n\n# Expected Output:\n# Found 22498 images belonging to 2 classes.\n# Found 2500 images belonging to 2 classes.\n","metadata":{"tags":[],"cell_id":"00006-d88efeee-6555-4eb1-8182-832618a43d59","deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Build Model\n\nmodel = tf.keras.models.Sequential([\n    tf.keras.layers.Conv2D(16, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n    tf.keras.layers.MaxPooling2D(2, 2),\n    tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),\n    tf.keras.layers.MaxPooling2D(2, 2),\n    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n    tf.keras.layers.MaxPooling2D(2, 2),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(512, activation='relu'),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\n\nmodel.compile(optimizer=RMSprop(lr=0.001), loss='binary_crossentropy', metrics=['acc'])","metadata":{"tags":[],"cell_id":"00005-f85178b9-e7cf-4f81-badc-7a44846198f6","deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00000-bdd3261b-f0c2-498e-9027-343c5465c6d5","deepnote_cell_type":"code"},"source":"# Note that this may take some time.\nhistory = model.fit_generator(train_generator,\n                              epochs=50,\n                              verbose=1,\n                              validation_data=validation_generator)\n\n","execution_count":null,"outputs":[]},{"cell_type":"code","source":"%matplotlib inline\n\nimport matplotlib.image  as mpimg\nimport matplotlib.pyplot as plt\n\n#-----------------------------------------------------------\n# Retrieve a list of list results on training and test data\n# sets for each training epoch\n#-----------------------------------------------------------\nacc=history.history['acc']\nval_acc=history.history['val_acc']\nloss=history.history['loss']\nval_loss=history.history['val_loss']\n\nepochs=range(len(acc)) # Get number of epochs\n\n#------------------------------------------------\n# Plot training and validation accuracy per epoch\n#------------------------------------------------\nplt.plot(epochs, acc, 'r', \"Training Accuracy\")\nplt.plot(epochs, val_acc, 'b', \"Validation Accuracy\")\nplt.title('Training and validation accuracy')\nplt.figure()\n\n#------------------------------------------------\n# Plot training and validation loss per epoch\n#------------------------------------------------\nplt.plot(epochs, loss, 'r', \"Training Loss\")\nplt.plot(epochs, val_loss, 'b', \"Validation Loss\")\nplt.figure()\n\n\n# Desired output. Charts with training and validation metrics. No crash :)","metadata":{"tags":[],"cell_id":"00008-d542a2a6-b177-4200-b475-a47ed0cdd35e","deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Here's a codeblock just for fun. You should be able to upload an image here \n# and have it classified without crashing\nimport numpy as np\nfrom google.colab import files\nfrom keras.preprocessing import image\n\nuploaded = files.upload()\n\nfor fn in uploaded.keys():\n \n  # predicting images\n  path = '/content/' + fn\n  img = image.load_img(path, target_size=(150, 150))\n  x = image.img_to_array(img)\n  x = np.expand_dims(x, axis=0)\n\n  images = np.vstack([x])\n  classes = model.predict(images, batch_size=10)\n  print(classes[0])\n  if classes[0]>0.5:\n    print(fn + \" is a dog\")\n  else:\n    print(fn + \" is a cat\") ","metadata":{"tags":[],"cell_id":"00009-20ebfbac-c0b8-4bd4-8bb3-6a839e77cf55","deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Exercise 3**\n\nPretrained Model and Transfer Learning  \nInception V3","metadata":{"tags":[],"cell_id":"00010-fc6cc007-c957-471a-8b57-a229191a7257","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"# Download the inception v3 weights\n!wget --no-check-certificate \\\n    https://storage.googleapis.com/mledu-datasets/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5 \\\n    -O /tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n\n# Import the inception model  \nfrom tensorflow.keras.applications.inception_v3 import InceptionV3\n\n# Create an instance of the inception model from the local pre-trained weights\nlocal_weights_file = '/tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5'\n\npre_trained_model = InceptionV3(input_shape = (150, 150, 3), \n                                include_top = False, \n                                weights = None)\n\npre_trained_model.load_weights(local_weights_file)\n\n# Make all the layers in the pre-trained model non-trainable\nfor layer in pre_trained_model.layers:\n  layer.trainable = False\n  \n# Print the model summary\npre_trained_model.summary()\n\n# Expected Output is extremely large, but should end with:\n\n#batch_normalization_v1_281 (Bat (None, 3, 3, 192)    576         conv2d_281[0][0]                 \n#__________________________________________________________________________________________________\n#activation_273 (Activation)     (None, 3, 3, 320)    0           batch_normalization_v1_273[0][0] \n#__________________________________________________________________________________________________\n#mixed9_1 (Concatenate)          (None, 3, 3, 768)    0           activation_275[0][0]             \n#                                                                 activation_276[0][0]             \n#__________________________________________________________________________________________________\n#concatenate_5 (Concatenate)     (None, 3, 3, 768)    0           activation_279[0][0]             \n#                                                                 activation_280[0][0]             \n#__________________________________________________________________________________________________\n#activation_281 (Activation)     (None, 3, 3, 192)    0           batch_normalization_v1_281[0][0] \n#__________________________________________________________________________________________________\n#mixed10 (Concatenate)           (None, 3, 3, 2048)   0           activation_273[0][0]             \n#                                                                 mixed9_1[0][0]                   \n#                                                                 concatenate_5[0][0]              \n#                                                                 activation_281[0][0]             \n#==================================================================================================\n#Total params: 21,802,784\n#Trainable params: 0\n#Non-trainable params: 21,802,784","metadata":{"tags":[],"cell_id":"00011-586a96fd-c60f-47f6-a1db-9745c212008f","deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"last_layer = pre_trained_model.get_layer('mixed7')\nprint('last layer output shape: ', last_layer.output_shape)\nlast_output = last_layer.output\n\n# Expected Output:\n# ('last layer output shape: ', (None, 7, 7, 768))","metadata":{"tags":[],"cell_id":"00012-73c38698-90a8-473a-a5b2-fd983e480eec","deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define a Callback class that stops training once accuracy reaches 99.9%\nclass myCallback(tf.keras.callbacks.Callback):\n  def on_epoch_end(self, epoch, logs={}):\n    if(logs.get('acc')>0.999):\n      print(\"\\nReached 99.9% accuracy so cancelling training!\")\n      self.model.stop_training = True","metadata":{"tags":[],"cell_id":"00013-48a97e4d-1245-48af-a9aa-69bfbcd5602a","deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.optimizers import RMSprop\n\n# Flatten the output layer to 1 dimension\nx = layers.Flatten()(last_output)\n# Add a fully connected layer with 1,024 hidden units and ReLU activation\nx = layers.Dense(1024, activation='relu')(x)\n# Add a dropout rate of 0.2\nx = layers.Dropout(0.2)(x)                  \n# Add a final sigmoid layer for classification\nx = layers.Dense  (1, activation='sigmoid')(x)           \n\nmodel = Model( pre_trained_model.input, x) \n\nmodel.compile(optimizer = RMSprop(lr=0.0001), \n              loss = 'binary_crossentropy', \n              metrics = ['acc'])\n\nmodel.summary()\n\n# Expected output will be large. Last few lines should be:\n\n# mixed7 (Concatenate)            (None, 7, 7, 768)    0           activation_248[0][0]             \n#                                                                  activation_251[0][0]             \n#                                                                  activation_256[0][0]             \n#                                                                  activation_257[0][0]             \n# __________________________________________________________________________________________________\n# flatten_4 (Flatten)             (None, 37632)        0           mixed7[0][0]                     \n# __________________________________________________________________________________________________\n# dense_8 (Dense)                 (None, 1024)         38536192    flatten_4[0][0]                  \n# __________________________________________________________________________________________________\n# dropout_4 (Dropout)             (None, 1024)         0           dense_8[0][0]                    \n# __________________________________________________________________________________________________\n# dense_9 (Dense)                 (None, 1)            1025        dropout_4[0][0]                  \n# ==================================================================================================\n# Total params: 47,512,481\n# Trainable params: 38,537,217\n# Non-trainable params: 8,975,264","metadata":{"tags":[],"cell_id":"00014-1880a564-a870-4674-9e08-550d386afa32","deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Get the Horse or Human dataset\n!wget --no-check-certificate https://storage.googleapis.com/laurencemoroney-blog.appspot.com/horse-or-human.zip -O /tmp/horse-or-human.zip\n\n# Get the Horse or Human Validation dataset\n!wget --no-check-certificate https://storage.googleapis.com/laurencemoroney-blog.appspot.com/validation-horse-or-human.zip -O /tmp/validation-horse-or-human.zip \n  \nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\nimport os\nimport zipfile\n\nlocal_zip = '//tmp/horse-or-human.zip'\nzip_ref = zipfile.ZipFile(local_zip, 'r')\nzip_ref.extractall('/tmp/training')\nzip_ref.close()\n\nlocal_zip = '//tmp/validation-horse-or-human.zip'\nzip_ref = zipfile.ZipFile(local_zip, 'r')\nzip_ref.extractall('/tmp/validation')\nzip_ref.close()","metadata":{"tags":[],"cell_id":"00015-520f3b3b-c8db-4259-a1ba-0251a60a3d0f","deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_horses_dir = os.path.join(train_dir, 'horses') # Directory with our training horse pictures\ntrain_humans_dir = os.path.join(train_dir, 'humans') # Directory with our training humans pictures\nvalidation_horses_dir = os.path.join(validation_dir, 'horses') # Directory with our validation horse pictures\nvalidation_humans_dir = os.path.join(validation_dir, 'humans')# Directory with our validation humanas pictures\n\ntrain_horses_fnames = os.listdir(train_horses_dir)\ntrain_humans_fnames = os.listdir(train_humans_dir)\nvalidation_horses_fnames = os.listdir(validation_horses_dir)\nvalidation_humans_fnames = os.listdir(validation_humans_dir)\n\nprint(len(train_horses_fnames))\nprint(len(train_humans_fnames))\nprint(len(validation_horses_fnames))\nprint(len(validation_humans_fnames))\n\n# Expected Output:\n# 500\n# 527\n# 128\n# 128","metadata":{"tags":[],"cell_id":"00016-663ee249-3b6f-4af4-8828-ca7e0b46baf0","deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define our example directories and files\ntrain_dir = '/tmp/training'\nvalidation_dir = '/tmp/validation'\n\n# Add our data-augmentation parameters to ImageDataGenerator\ntrain_datagen = ImageDataGenerator(rescale = 1./255.,\n                                   rotation_range = 40,\n                                   width_shift_range = 0.2,\n                                   height_shift_range = 0.2,\n                                   shear_range = 0.2,\n                                   zoom_range = 0.2,\n                                   horizontal_flip = True)\n\n# Note that the validation data should not be augmented!\ntest_datagen = ImageDataGenerator( rescale = 1.0/255. )\n\n# Flow training images in batches of 20 using train_datagen generator\ntrain_generator = train_datagen.flow_from_directory(train_dir,\n                                                    batch_size = 20,\n                                                    class_mode = 'binary', \n                                                    target_size = (150, 150))     \n\n# Flow validation images in batches of 20 using test_datagen generator\nvalidation_generator =  test_datagen.flow_from_directory( validation_dir,\n                                                          batch_size  = 20,\n                                                          class_mode  = 'binary', \n                                                          target_size = (150, 150))","metadata":{"tags":[],"cell_id":"00017-0c70aa7a-b047-403a-920c-6722bf375baa","deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Run this and see how many epochs it should take before the callback\n# fires, and stops training at 99.9% accuracy\n# (It should take less than 100 epochs)\ncallbacks = myCallback()\nhistory = model.fit_generator(\n            train_generator,\n            validation_data = validation_generator,\n            steps_per_epoch = 100,\n            epochs = 100,\n            validation_steps = 50,\n            verbose = 2,\n            callbacks=[callbacks])","metadata":{"tags":[],"cell_id":"00018-62fc80e1-7712-4424-8cef-09876a07a5b0","deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nacc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(len(acc))\n\nplt.plot(epochs, acc, 'r', label='Training accuracy')\nplt.plot(epochs, val_acc, 'b', label='Validation accuracy')\nplt.title('Training and validation accuracy')\nplt.legend(loc=0)\nplt.figure()\n\n\nplt.show()","metadata":{"tags":[],"cell_id":"00019-3d66b1a2-c01f-4abc-8bb0-1740f2c083ab","deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Exercise 4**\n\nThe data for this exercise is available at: https://www.kaggle.com/datamunge/sign-language-mnist/home\n\nSign up and download to find 2 CSV files: sign_mnist_test.csv and sign_mnist_train.csv -- You will upload both of them using this button before you can continue.","metadata":{"tags":[],"cell_id":"00020-64a54a88-0e95-4904-977e-41b6074a9319","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"import csv\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom google.colab import files","metadata":{"tags":[],"cell_id":"00021-a54b07a4-3874-42c1-8ca2-4da3324681dc","deepnote_to_be_reexecuted":false,"source_hash":null,"execution_start":1613509827648,"execution_millis":4805,"output_cleared":true,"deepnote_cell_type":"code"},"outputs":[],"execution_count":2},{"cell_type":"code","source":"uploaded=files.upload()","metadata":{"tags":[],"cell_id":"00022-c3478e6a-a952-44a3-88b1-e2a070a10e2c","deepnote_to_be_reexecuted":false,"source_hash":null,"execution_start":1613509818137,"execution_millis":439,"output_cleared":true,"deepnote_cell_type":"code"},"outputs":[],"execution_count":1},{"cell_type":"code","source":"def get_data(filename):\n    with open(filename) as training_file:\n        csv_reader = csv.reader(training_file, delimiter=',')\n        first_line = True\n        temp_images = []\n        temp_labels = []\n        for row in csv_reader:\n            if first_line:\n                # print(\"Ignoring first line\")\n                first_line = False\n            else:\n                temp_labels.append(row[0])\n                image_data = row[1:785]\n                image_data_as_array = np.array_split(image_data, 28)\n                temp_images.append(image_data_as_array)\n        images = np.array(temp_images).astype('float')\n        labels = np.array(temp_labels).astype('float')\n    return images, labels\n\n\ntraining_images, training_labels = get_data('sign_mnist_train.csv')\ntesting_images, testing_labels = get_data('sign_mnist_test.csv')\n\nprint(training_images.shape)\nprint(training_labels.shape)\nprint(testing_images.shape)\nprint(testing_labels.shape)\n","metadata":{"tags":[],"cell_id":"00023-1f4d6e4e-b39f-470e-ad9e-c319ad25c8f4","deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"training_images = np.expand_dims(training_images, axis=3)\ntesting_images = np.expand_dims(testing_images, axis=3)\n\ntrain_datagen = ImageDataGenerator(\n    rescale=1. / 255,\n    rotation_range=40,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    fill_mode='nearest')\n\nvalidation_datagen = ImageDataGenerator(\n    rescale=1. / 255)\n\nprint(training_images.shape)\nprint(testing_images.shape)","metadata":{"tags":[],"cell_id":"00024-961c3051-4eb1-4fce-a7e2-b454dbe75612","deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = tf.keras.models.Sequential([\n    tf.keras.layers.Conv2D(64, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n    tf.keras.layers.MaxPooling2D(2, 2),\n    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n    tf.keras.layers.MaxPooling2D(2, 2),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n    tf.keras.layers.Dense(26, activation=tf.nn.softmax)])\n\nmodel.compile(optimizer = tf.train.AdamOptimizer(),\n              loss = 'sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n\nhistory = model.fit_generator(train_datagen.flow(training_images, training_labels, batch_size=32),\n                              steps_per_epoch=len(training_images) / 32,\n                              epochs=15,\n                              validation_data=validation_datagen.flow(testing_images, testing_labels, batch_size=32),\n                              validation_steps=len(testing_images) / 32)\n\nmodel.evaluate(testing_images, testing_labels)","metadata":{"tags":[],"cell_id":"00025-129c6ca9-516c-448d-adc0-e90e69d17aa3","deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nacc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(len(acc))\n\nplt.plot(epochs, acc, 'r', label='Training accuracy')\nplt.plot(epochs, val_acc, 'b', label='Validation accuracy')\nplt.title('Training and validation accuracy')\nplt.legend()\nplt.figure()\n\nplt.plot(epochs, loss, 'r', label='Training Loss')\nplt.plot(epochs, val_loss, 'b', label='Validation Loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()","metadata":{"tags":[],"cell_id":"00026-668eec89-96b1-4beb-aee2-ded23bcbd5d9","deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=ca6dbea1-488b-41fc-b438-74aac24ec6dc' target=\"_blank\">\n<img style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"tags":[],"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":2,"metadata":{"orig_nbformat":2,"deepnote_notebook_id":"ddec26db-1c08-4d5e-8ca3-c4e8ca1d7e35","deepnote":{},"deepnote_execution_queue":[]}}